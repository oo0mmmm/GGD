{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import BatchSampler, RandomSampler, WeightedRandomSampler\n",
    "from torch import linalg as la\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "#Imports ijcnn dataset in libsvm format\n",
    "class LIBSVMdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, n_features, n_sizes):\n",
    "        self.file_path = file_path\n",
    "        self.n_features = n_features\n",
    "        self.n_sizes = n_sizes\n",
    "        with open(self.file_path, \"r\") as fp:\n",
    "            self.dmatrix = fp.readlines()\n",
    "        \n",
    "    def label_transformer(self, l):\n",
    "        if l != 1:\n",
    "            l = 0\n",
    "        return l\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(self.label_transformer(float(line[0]))), line[1:]\n",
    "        value = torch.zeros((self.n_features))\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx)-1] = float(val)\n",
    "        return [label, value]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.n_sizes)\n",
    "    \n",
    "    def __getitem__(self, number):\n",
    "        target, features = self.process_line(self.dmatrix[number].strip(\"\\n\"))\n",
    "        return target, features\n",
    "\n",
    "\n",
    "#Resets gradients of all model parameters\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "            \n",
    "#Records the training losses\n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the testing losses\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for y, x in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the l2 norm of gradients\n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "\n",
    "#Records the full gradients (used in SVRG and GGD-SVRG algorithms)\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad # a list of model parameters\n",
    "\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "\n",
    "#preparation for training \n",
    "d = 22 \n",
    "n = 49990 \n",
    "n1 = 91701\n",
    "#Regularization parameter\n",
    "weight_decay = 1/n\n",
    "epoches = 30\n",
    "rec = 512\n",
    "#Loss function that is used to record the experiment results.\n",
    "loss_func_rec = nn.BCELoss(reduction = 'sum')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Datasets which are used for training and testing respectively\n",
    "ijcnn_train = LIBSVMdataset(\"./ijcnn1\", d, n)\n",
    "ijcnn_test = LIBSVMdataset(\"./ijcnn1.t\", d, n1)\n",
    "#Dataloaders which are used to record the training, testing losses and l2-norm of full gradients.\n",
    "ijcnn_train_loader = DataLoader(ijcnn_train, batch_size = rec)\n",
    "ijcnn_train_loader = DeviceDataLoader(ijcnn_train_loader, device)\n",
    "ijcnn_test_loader = DataLoader(ijcnn_test, batch_size = rec)\n",
    "ijcnn_test_loader = DeviceDataLoader(ijcnn_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-as\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.6\n",
    "#Setting lr_schedule = 'constant' can obtain the experiment results for GGD\n",
    "lr_schedule = 't-inverse'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/(b*m))\n",
    "ijcnn_ggdas_loss_list = []\n",
    "ijcnn_ggdas_gradnorm_list = []\n",
    "ijcnn_ggdas_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/(1 + int(epoch/(3*max_batch_size)) ))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = LR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(LR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)    \n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_ggdas_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_ggdas_test_loss_list.append(current_test_loss)\n",
    "        current_iteration =  int((epoch+1)/max_batch_size)\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-adam \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.005\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/(b*m))\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "\n",
    "ijcnn_gadam_loss_list = []\n",
    "ijcnn_gadam_gradnorm_list = []\n",
    "ijcnn_gadam_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "batch_idx = torch.tensor(0)\n",
    "h_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = LR_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2)  in enumerate(zip(list(LR_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_gadam_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_gadam_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_gadam_test_loss_list.append(current_test_loss)\n",
    "            current_iteration =  (epoch+1)/max_batch_size\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-svrg\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "Snap_model = copy.deepcopy(LR_net)\n",
    "LR_net.to(device)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.8\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/(b*m))\n",
    "q = int(0.26*n)\n",
    "\n",
    "\n",
    "ijcnn_gsvrg_loss_list = []\n",
    "ijcnn_gsvrg_gradnorm_list = []\n",
    "ijcnn_gsvrg_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "fg = full_grad(Snap_model, loss_func_rec, ijcnn_train_loader, n)\n",
    "batch_idx = 0\n",
    "\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = LR_net(x)\n",
    "            loss = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(LR_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "       \n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(LR_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equals to update frequency\n",
    "    if  batch_idx  % q == 0:\n",
    "        Snap_model = copy.deepcopy(LR_net)\n",
    "        fg = full_grad(Snap_model, loss_func_rec, ijcnn_train_loader, n)\n",
    "            \n",
    "    if batch_idx % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_gsvrg_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_gsvrg_test_loss_list.append(current_test_loss)\n",
    "            current_iteration = batch_idx / max_batch_size\n",
    "            print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
