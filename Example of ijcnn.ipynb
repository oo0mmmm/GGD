{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36748ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Example of logistic regression for ijcnn1 dataset. \n",
    "#Experiment results using other datasets can be obtained similarily.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import BatchSampler, RandomSampler, WeightedRandomSampler\n",
    "from torch import linalg as la\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "#Imports ijcnn dataset in libsvm format\n",
    "class LIBSVMdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, n_features, n_sizes):\n",
    "        self.file_path = file_path\n",
    "        self.n_features = n_features\n",
    "        self.n_sizes = n_sizes\n",
    "        with open(self.file_path, \"r\") as fp:\n",
    "            self.dmatrix = fp.readlines()\n",
    "        \n",
    "    def label_transformer(self, l):\n",
    "        if l != 1:\n",
    "            l = 0\n",
    "        return l\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(self.label_transformer(float(line[0]))), line[1:]\n",
    "        value = torch.zeros((self.n_features))\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx)-1] = float(val)\n",
    "        return [label, value]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.n_sizes)\n",
    "    \n",
    "    def __getitem__(self, number):\n",
    "        target, features = self.process_line(self.dmatrix[number].strip(\"\\n\"))\n",
    "        return target, features\n",
    "\n",
    "    \n",
    "#This subclass is speicalized for a9a, covtype and rcv1 where line[1:] is changed to line[1:-1]\n",
    "class LIBSVMdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, n_features, n_sizes):\n",
    "        self.file_path = file_path\n",
    "        self.n_features = n_features\n",
    "        self.n_sizes = n_sizes\n",
    "        with open(self.file_path, \"r\") as fp:\n",
    "            self.dmatrix = fp.readlines()\n",
    "        \n",
    "    def label_transformer(self, l):\n",
    "        if l != 1:\n",
    "            l = 0\n",
    "        return l\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(self.label_transformer(float(line[0]))), line[1:-1]\n",
    "        value = torch.zeros((self.n_features))\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx)-1] = float(val)\n",
    "        return [label, value]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.n_sizes)\n",
    "    \n",
    "    def __getitem__(self, number):\n",
    "        target, features = self.process_line(self.dmatrix[number].strip(\"\\n\"))\n",
    "        return target, features\n",
    "\n",
    "\n",
    "#Resets gradients of all model parameters\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "            \n",
    "#Records the training losses\n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the testing losses\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for y, x in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the l2 norm of gradients\n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "\n",
    "#Records the full gradients (used in SVRG and GGD-SVRG algorithms)\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad # a list of model parameters\n",
    "\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "\n",
    "#preparation for training \n",
    "d = 22 \n",
    "n = 49990 \n",
    "n1 = 91701\n",
    "#Regularization parameter\n",
    "weight_decay = 1/n\n",
    "epoches = 30\n",
    "rec = 512\n",
    "#Loss function that is used to record the experiment results.\n",
    "loss_func_rec = nn.BCELoss(reduction = 'sum')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Datasets which are used for training and testing respectively\n",
    "ijcnn_train = LIBSVMdataset(\"./ijcnn1\", d, n)\n",
    "ijcnn_test = LIBSVMdataset(\"./ijcnn1.t\", d, n1)\n",
    "#Dataloaders which are used to record the training, testing losses and l2-norm of full gradients.\n",
    "ijcnn_train_loader = DataLoader(ijcnn_train, batch_size = rec)\n",
    "ijcnn_train_loader = DeviceDataLoader(ijcnn_train_loader, device)\n",
    "ijcnn_test_loader = DataLoader(ijcnn_test, batch_size = rec)\n",
    "ijcnn_test_loader = DeviceDataLoader(ijcnn_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9793c454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020012855529785156\n"
     ]
    }
   ],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.3\n",
    "#Setting lr_schedule = 'constant' can obtain the experiment results for GGD\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/m)\n",
    "\n",
    "\n",
    "#Records the experiment results\n",
    "ijcnn_ggd_loss_list = []\n",
    "ijcnn_ggd_gradnorm_list = []\n",
    "ijcnn_ggd_test_loss_list = []\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #Calculates loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/(1 + int(epoch/(3*max_batch_size)) ))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #Constructs the grafting gradient\n",
    "        output1 = LR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(LR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)\n",
    "    #Records the experiment results\n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_ggd_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_ggd_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_ggd_test_loss_list.append(current_test_loss)\n",
    "        current_iteration =  int((epoch+1)/max_batch_size)\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac40bbd0-9bac-463b-aa44-a1e96312e948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Loss: 0.1982575617767848  Gradnorm:0.007247020490467548\n",
      "Iteration: 2  Loss: 0.1896725509055045  Gradnorm:0.009511294774711132\n",
      "Iteration: 3  Loss: 0.18559380954728896  Gradnorm:0.003060684073716402\n",
      "Iteration: 4  Loss: 0.18510871566631304  Gradnorm:0.004141435492783785\n",
      "Iteration: 5  Loss: 0.184926284597409  Gradnorm:0.0032713182736188173\n",
      "Iteration: 6  Loss: 0.1847358411626079  Gradnorm:0.00452456483617425\n",
      "Iteration: 7  Loss: 0.18408259482770678  Gradnorm:0.0013396989088505507\n",
      "Iteration: 8  Loss: 0.18414956665396157  Gradnorm:0.0018972775433212519\n",
      "Iteration: 9  Loss: 0.18402296759377323  Gradnorm:0.0013600385282188654\n",
      "Iteration: 10  Loss: 0.18444590975241268  Gradnorm:0.008082958869636059\n",
      "Iteration: 11  Loss: 0.18385079044130256  Gradnorm:0.0009369252948090434\n",
      "Iteration: 12  Loss: 0.18413455821289876  Gradnorm:0.003958773799240589\n",
      "Iteration: 13  Loss: 0.1839299693363884  Gradnorm:0.0016152426833286881\n",
      "Iteration: 14  Loss: 0.1839739313210481  Gradnorm:0.001546463230624795\n",
      "Iteration: 15  Loss: 0.18393396861122555  Gradnorm:0.0011271134717389941\n",
      "Iteration: 16  Loss: 0.18392963134853516  Gradnorm:0.0023780260235071182\n",
      "Iteration: 17  Loss: 0.18387099354657288  Gradnorm:0.001139137428253889\n",
      "Iteration: 18  Loss: 0.18397657000768303  Gradnorm:0.0017067180015146732\n",
      "Iteration: 19  Loss: 0.18390123987242912  Gradnorm:0.0036456447560340166\n",
      "Iteration: 20  Loss: 0.18389837227198125  Gradnorm:0.0025871533434838057\n",
      "Iteration: 21  Loss: 0.18388177824731083  Gradnorm:0.0013828625669702888\n",
      "Iteration: 22  Loss: 0.18378822541363493  Gradnorm:0.001961961854249239\n",
      "Iteration: 23  Loss: 0.18389614108619412  Gradnorm:0.0032556764781475067\n",
      "Iteration: 24  Loss: 0.18379628226255282  Gradnorm:0.0012867024634033442\n",
      "Iteration: 25  Loss: 0.18384870007295742  Gradnorm:0.0012460004072636366\n",
      "Iteration: 26  Loss: 0.18393650494817956  Gradnorm:0.004391265567392111\n",
      "Iteration: 27  Loss: 0.1837605465345038  Gradnorm:0.0010588552104309201\n",
      "Iteration: 28  Loss: 0.18373978258505824  Gradnorm:0.0011174995452165604\n",
      "Iteration: 29  Loss: 0.183803079795431  Gradnorm:0.0035241227596998215\n",
      "Iteration: 30  Loss: 0.18375302043325845  Gradnorm:0.0028098267503082752\n"
     ]
    }
   ],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-as\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.6\n",
    "#Setting lr_schedule = 'constant' can obtain the experiment results for GGD\n",
    "lr_schedule = 't-inverse'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/m)\n",
    "ijcnn_ggdas_loss_list = []\n",
    "ijcnn_ggdas_gradnorm_list = []\n",
    "ijcnn_ggdas_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/(1 + int(epoch/(3*max_batch_size)) ))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = LR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(LR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)    \n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_ggdas_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_ggdas_test_loss_list.append(current_test_loss)\n",
    "        current_iteration =  int((epoch+1)/max_batch_size)\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4fbaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1.0  Loss: 0.2073433837744723  Gradnorm:0.005959228612482548\n",
      "Iteration: 2.0  Loss: 0.1931979152265279  Gradnorm:0.007480834610760212\n",
      "Iteration: 3.0  Loss: 0.1888423987060375  Gradnorm:0.0034978955518454313\n",
      "Iteration: 4.0  Loss: 0.1865093133909195  Gradnorm:0.00254568038508296\n",
      "Iteration: 5.0  Loss: 0.18518206292883987  Gradnorm:0.001739699044264853\n",
      "Iteration: 6.0  Loss: 0.1848588739550433  Gradnorm:0.0027744704857468605\n",
      "Iteration: 7.0  Loss: 0.18431534514654482  Gradnorm:0.0017786796670407057\n",
      "Iteration: 8.0  Loss: 0.18445239953388304  Gradnorm:0.0037267901934683323\n",
      "Iteration: 9.0  Loss: 0.18395542650138136  Gradnorm:0.0030430860351771116\n",
      "Iteration: 10.0  Loss: 0.1838087648454481  Gradnorm:0.00368976267054677\n",
      "Iteration: 11.0  Loss: 0.18430912385190368  Gradnorm:0.005772940814495087\n",
      "Iteration: 12.0  Loss: 0.18410606713699884  Gradnorm:0.0016819594893604517\n",
      "Iteration: 13.0  Loss: 0.18430746871468298  Gradnorm:0.004648223053663969\n",
      "Iteration: 14.0  Loss: 0.18364280414812745  Gradnorm:0.0013103327946737409\n",
      "Iteration: 15.0  Loss: 0.18424047310001973  Gradnorm:0.007408252917230129\n",
      "Iteration: 16.0  Loss: 0.18390835811711886  Gradnorm:0.00312284124083817\n",
      "Iteration: 17.0  Loss: 0.18397542237119482  Gradnorm:0.0032371855340898037\n",
      "Iteration: 18.0  Loss: 0.1839938300783806  Gradnorm:0.002045372501015663\n",
      "Iteration: 19.0  Loss: 0.18405824216288225  Gradnorm:0.0030347949359565973\n",
      "Iteration: 20.0  Loss: 0.1840049253440181  Gradnorm:0.003644973272457719\n",
      "Iteration: 21.0  Loss: 0.183951590951527  Gradnorm:0.004259388893842697\n",
      "Iteration: 22.0  Loss: 0.18380500953629994  Gradnorm:0.0018393322825431824\n",
      "Iteration: 23.0  Loss: 0.1836865962385513  Gradnorm:0.004817885346710682\n",
      "Iteration: 24.0  Loss: 0.18417330131961754  Gradnorm:0.00827593170106411\n",
      "Iteration: 25.0  Loss: 0.1838573335598896  Gradnorm:0.00476430868729949\n",
      "Iteration: 26.0  Loss: 0.1836859036718249  Gradnorm:0.0018911598017439246\n",
      "Iteration: 27.0  Loss: 0.18361831150814056  Gradnorm:0.0032366602681577206\n",
      "Iteration: 28.0  Loss: 0.18391814945415969  Gradnorm:0.0031223304104059935\n",
      "Iteration: 29.0  Loss: 0.1839166433213267  Gradnorm:0.002708909334614873\n",
      "Iteration: 30.0  Loss: 0.18385987163288586  Gradnorm:0.0038131324108690023\n"
     ]
    }
   ],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-adam \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.005\n",
    "lr_schedule = 'constant'\n",
    "max_batch_size = int(n/m)\n",
    "b = 2\n",
    "m = 16\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "\n",
    "ijcnn_gadam_loss_list = []\n",
    "ijcnn_gadam_gradnorm_list = []\n",
    "ijcnn_gadam_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "batch_idx = torch.tensor(0)\n",
    "h_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = LR_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2)  in enumerate(zip(list(LR_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_gadam_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_gadam_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_gadam_test_loss_list.append(current_test_loss)\n",
    "            current_iteration =  (epoch+1)/max_batch_size\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b761ac5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1.0  Loss: 0.19911395534396745  Gradnorm:0.00921198446303606\n",
      "Iteration: 2.0  Loss: 0.18602391159006787  Gradnorm:0.003657327964901924\n",
      "Iteration: 3.0  Loss: 0.18404181968446112  Gradnorm:0.0011176865082234144\n",
      "Iteration: 4.0  Loss: 0.1834598397700759  Gradnorm:0.00041204976150766015\n",
      "Iteration: 5.0  Loss: 0.18325303920439584  Gradnorm:0.00023144188162405044\n",
      "Iteration: 6.0  Loss: 0.18317369467672137  Gradnorm:0.00015518673171754926\n",
      "Iteration: 7.0  Loss: 0.18314203874607407  Gradnorm:9.896888514049351e-05\n",
      "Iteration: 8.0  Loss: 0.18312935426516136  Gradnorm:6.760034739272669e-05\n",
      "Iteration: 9.0  Loss: 0.18312396953493654  Gradnorm:4.660655031329952e-05\n",
      "Iteration: 10.0  Loss: 0.18312172592723402  Gradnorm:2.5679766622488387e-05\n",
      "Iteration: 11.0  Loss: 0.18312079719277372  Gradnorm:2.8385889891069382e-05\n",
      "Iteration: 12.0  Loss: 0.1831204056987735  Gradnorm:1.333090040134266e-05\n",
      "Iteration: 13.0  Loss: 0.18312024370303007  Gradnorm:7.986426680872682e-06\n",
      "Iteration: 14.0  Loss: 0.18312017319503576  Gradnorm:6.588170890609035e-06\n",
      "Iteration: 15.0  Loss: 0.18312014417590775  Gradnorm:2.9673337849089876e-06\n",
      "Iteration: 16.0  Loss: 0.18312012988939433  Gradnorm:1.833776309467794e-06\n",
      "Iteration: 17.0  Loss: 0.1831201261864493  Gradnorm:1.5012270750958123e-06\n",
      "Iteration: 18.0  Loss: 0.18312012480583645  Gradnorm:1.2055813840561314e-06\n",
      "Iteration: 19.0  Loss: 0.18312012280434178  Gradnorm:9.677842172095552e-07\n",
      "Iteration: 20.0  Loss: 0.18312012293948896  Gradnorm:9.54228880800656e-07\n",
      "Iteration: 21.0  Loss: 0.18312012293948907  Gradnorm:9.540590326651e-07\n",
      "Iteration: 22.0  Loss: 0.18312012293948898  Gradnorm:9.540033261146164e-07\n",
      "Iteration: 23.0  Loss: 0.1831201229394891  Gradnorm:9.539160146232462e-07\n",
      "Iteration: 24.0  Loss: 0.18312012293948932  Gradnorm:9.53909989220847e-07\n",
      "Iteration: 25.0  Loss: 0.18312012293948907  Gradnorm:9.547278523314162e-07\n",
      "Iteration: 26.0  Loss: 0.1831201229394892  Gradnorm:9.542155794406426e-07\n",
      "Iteration: 27.0  Loss: 0.1831201229394892  Gradnorm:9.541437293592026e-07\n",
      "Iteration: 28.0  Loss: 0.1831201229394891  Gradnorm:9.53951598603453e-07\n",
      "Iteration: 29.0  Loss: 0.18312012293948904  Gradnorm:9.534443279335392e-07\n",
      "Iteration: 30.0  Loss: 0.18312012293948932  Gradnorm:9.539601251162821e-07\n"
     ]
    }
   ],
   "source": [
    "#training stage: logistic regression with l^2 regularization using ggd-svrg\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_net = LogisticRegression(input_dim = d, output_dim = 1)\n",
    "Snap_model = copy.deepcopy(LR_net)\n",
    "LR_net.to(device)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "lr0 = 0.8\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 16\n",
    "max_batch_size = int(n/m)\n",
    "q = 0.26*max_batch_size\n",
    "\n",
    "\n",
    "ijcnn_gsvrg_loss_list = []\n",
    "ijcnn_gsvrg_gradnorm_list = []\n",
    "ijcnn_gsvrg_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(RandomSampler(ijcnn_train, replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_ijcnn_train_loader = DataLoader(ijcnn_train, batch_sampler = BS)\n",
    "LR_ijcnn_train_loader = DeviceDataLoader(LR_ijcnn_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "fg = full_grad(Snap_model, loss_func_rec, ijcnn_train_loader, n)\n",
    "batch_idx = 0\n",
    "\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for y_target, x_data in LR_ijcnn_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = LR_net(x)\n",
    "            loss = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(LR_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "       \n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(LR_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equals to update frequency\n",
    "    if  batch_idx  % q == 0:\n",
    "        Snap_model = copy.deepcopy(LR_net)\n",
    "        fg = full_grad(Snap_model, loss_func_rec, ijcnn_train_loader, n)\n",
    "            \n",
    "    if batch_idx % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "        ijcnn_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, ijcnn_train_loader, n)\n",
    "            ijcnn_gsvrg_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, ijcnn_test_loader, n1)\n",
    "            ijcnn_gsvrg_test_loss_list.append(current_test_loss)\n",
    "            current_iteration = batch_idx / max_batch_size\n",
    "            print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
