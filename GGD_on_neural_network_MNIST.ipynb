{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5026cc2-0ef7-40a5-83a4-7c6cfc34d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import linalg as la\n",
    "\n",
    "#Set a simple cnn for classifying mnist datasets\n",
    "class MNIST_net(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super(MNIST_net, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16*5*5,120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, self.classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for x, y in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    "        \n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad # a list of model parameters\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "train_trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_trans = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "MNIST_train = MNIST(root='test/', train = True, download=True, transform=train_trans) \n",
    "MNIST_test = MNIST(root='test/', train = False, download=True, transform=test_trans) \n",
    "n = len(MNIST_train)\n",
    "n1 = len(MNIST_test)\n",
    "weight_decay = 1e-4\n",
    "nabla = 1e-4\n",
    "rbs = 1000\n",
    "epoches = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MNIST_train_loader_eva = DataLoader(MNIST_train, shuffle = False, batch_size = rbs)\n",
    "MNIST_train_loader_eva = DeviceDataLoader(MNIST_train_loader_eva, device)\n",
    "MNIST_test_loader_eva = DataLoader(MNIST_test, shuffle = False, batch_size = rbs)\n",
    "MNIST_test_loader_eva = DeviceDataLoader(MNIST_test_loader_eva, device)\n",
    "loss_func_rec = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9fc14-0679-4af0-9517-3d4fb490b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: mnist training with l^2 regularization using ggd-adamas (two models version)\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "CNN_MNIST_net = MNIST_net()\n",
    "CNN_MNIST_net = CNN_MNIST_net.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#GPU TRAINING if possible\n",
    "copied_model = copy.deepcopy(CNN_MNIST_net)\n",
    "copied_model.to(device)\n",
    "\n",
    "ilr0 = 1e-4\n",
    "flr0 = 1e-6\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "MNIST_gadamas_loss_list = []\n",
    "MNIST_gadamas_gradnorm_list = []\n",
    "MNIST_gadamas_test_loss_list = []\n",
    "MNIST_gadamas_iteration_list = []\n",
    "\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "#stage two: load training set\n",
    "\n",
    "MNIST_train_loader = DataLoader(MNIST_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "MNIST_train_loader = DeviceDataLoader(MNIST_train_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_idx = torch.tensor(0)\n",
    "#stage three: train and test \n",
    "h_0 = [torch.zeros_like(paras) for paras in list(CNN_MNIST_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(CNN_MNIST_net.parameters())]\n",
    "for epoch in range(epoches):\n",
    "    CNN_MNIST_net.train()\n",
    "    \n",
    "    for x_data, y_target in MNIST_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch )\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = CNN_MNIST_net(x)\n",
    "                losst[i] = loss_func(output, yt[i]).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(CNN_MNIST_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = CNN_MNIST_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0])\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1])\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2) in enumerate(zip(list(CNN_MNIST_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(CNN_MNIST_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    CNN_MNIST_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "    MNIST_gadamas_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "        MNIST_gadamas_loss_list.append(current_loss)\n",
    "        MNIST_gadamas_test_loss_list.append(test_loss(CNN_MNIST_net, loss_func_rec, MNIST_test_loader_eva, n1))\n",
    "        current_iteration =  epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31a81e-a08e-44d3-b2ac-00abeb78422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: mnist training with l^2 regularization using ggdas (two model version)\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "CNN_MNIST_net = MNIST_net()\n",
    "CNN_MNIST_net = CNN_MNIST_net.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#GPU TRAINING if possible\n",
    "copied_model = copy.deepcopy(CNN_MNIST_net)\n",
    "copied_model.to(device)\n",
    "\n",
    "ilr0 = 0.01\n",
    "flr0 = 1e-5\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "\n",
    "MNIST_ggdas_loss_list = []\n",
    "MNIST_ggdas_gradnorm_list = []\n",
    "MNIST_ggdas_test_loss_list = []\n",
    "MNIST_ggdas_iteration_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "\n",
    "MNIST_train_loader = DataLoader(MNIST_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "MNIST_train_loader = DeviceDataLoader(MNIST_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    CNN_MNIST_net.train()\n",
    "    for x_data, y_target in MNIST_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = CNN_MNIST_net(x)\n",
    "                losst[i] = loss_func(output, yt[i]).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(CNN_MNIST_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch)\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = CNN_MNIST_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0])\n",
    "        loss2 = loss_func(output2, yt[1])\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(CNN_MNIST_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(CNN_MNIST_net)\n",
    "        copied_model.to(device)    \n",
    "    CNN_MNIST_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "    MNIST_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "        MNIST_ggdas_loss_list.append(current_loss)\n",
    "        MNIST_ggdas_test_loss_list.append(test_loss(CNN_MNIST_net, loss_func_rec, MNIST_test_loader_eva, n1))\n",
    "        current_iteration =  epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af5713-42a5-4e92-a86b-eb2c73e6a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: mnist training with ggd-svrg-as\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "CNN_MNIST_net = MNIST_net()\n",
    "CNN_MNIST_net = CNN_MNIST_net.to(device)\n",
    "Snap_model = copy.deepcopy(CNN_MNIST_net)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#initial learning rate\n",
    "ilr0 = 100/(n ** (2/3))\n",
    "#final learning rate\n",
    "flr0 = 1/(n ** (2/3))\n",
    "#learning schedule\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "MNIST_gsvrg_loss_list = []\n",
    "MNIST_gsvrg_gradnorm_list = []\n",
    "MNIST_gsvrg_test_loss_list = []\n",
    "\n",
    "q = 3*int(n/m) #update frequency\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "MNIST_train_loader = DataLoader(MNIST_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "MNIST_train_loader = DeviceDataLoader(MNIST_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test\n",
    "fg = full_grad(Snap_model, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "batch_idx = 0 \n",
    "for epoch in range(epoches):\n",
    "    CNN_MNIST_net.train()\n",
    "    for x_data, y_target in MNIST_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch)\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #part one: prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i])\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = CNN_MNIST_net(x)\n",
    "            loss = loss_func(output, yt[i])\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(CNN_MNIST_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(CNN_MNIST_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(CNN_MNIST_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "       \n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(CNN_MNIST_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equal update frequency\n",
    "        if  batch_idx  % q == 0:\n",
    "            Snap_model = copy.deepcopy(CNN_MNIST_net)\n",
    "            fg = full_grad(Snap_model, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "    CNN_MNIST_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "    MNIST_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_MNIST_net, loss_func_rec, MNIST_train_loader_eva, n)\n",
    "        MNIST_gsvrg_loss_list.append(current_loss)\n",
    "        MNIST_gsvrg_test_loss_list.append(test_loss(CNN_MNIST_net, loss_func_rec, MNIST_test_loader_eva, n1))\n",
    "        current_iteration = epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
