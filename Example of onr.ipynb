{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19b4953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, RandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch import linalg as la\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        return x  \n",
    "    \n",
    "\n",
    "#Reads Onr dataset\n",
    "class CSVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = dataset.iloc[:,:-1].values\n",
    "        self.labels = dataset.iloc[:,-1].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        feature = torch.tensor(feature, dtype = torch.float)\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype = torch.float)\n",
    "        return feature, label\n",
    "    \n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "            \n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for x, y in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    " \n",
    "    \n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad \n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "#Reads Onr dataset, scales and transforms it into class of 'torch.dataset'\n",
    "onr_data = pd.read_csv('OnlineNewsPopularity.csv', dtype = np.float32)\n",
    "MaxminScaler = preprocessing.MinMaxScaler()\n",
    "onr_data = MaxminScaler.fit_transform(onr_data)\n",
    "onr_data = pd.DataFrame(onr_data)\n",
    "onr_dataset = CSVDataset(onr_data)     \n",
    "d = 58\n",
    "nt = 39644\n",
    "n = 31715\n",
    "n1 = nt - n\n",
    "epoches = 100\n",
    "weight_decay = 1e-6\n",
    "rbs = 500\n",
    "#Loss function that is used to record the training, testing losses and l2-norm of full gradients\n",
    "loss_func_rec = nn.MSELoss(reduction='sum')\n",
    "#Split the Onr dataset into training set and testing set.\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "onr_train, onr_test = torch.utils.data.random_split(onr_dataset, [n,n1], generator = generator1)\n",
    "#Dataloaders which are used to calculate the training, testing losses and l2-norm of full gradients\n",
    "onr_train_loader = DataLoader(onr_train, batch_size = rbs)\n",
    "onr_train_loader = DeviceDataLoader(onr_train_loader, device)\n",
    "onr_test_loader = DataLoader(onr_test, batch_size = rbs)\n",
    "onr_test_loader = DeviceDataLoader(onr_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda45ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1.0  Loss: 0.004618581950467185  Gradnorm:0.04206166788935661\n",
      "Iteration: 2.0  Loss: 0.0018463852276622394  Gradnorm:0.024229800328612328\n",
      "Iteration: 3.0  Loss: 0.0009843065002711436  Gradnorm:0.00929498765617609\n",
      "Iteration: 4.0  Loss: 0.0006025709861388215  Gradnorm:0.006281221751123667\n",
      "Iteration: 5.0  Loss: 0.0004097160479914554  Gradnorm:0.0032170673366636038\n",
      "Iteration: 6.0  Loss: 0.00032122556355667476  Gradnorm:0.0047879875637590885\n",
      "Iteration: 7.0  Loss: 0.00026979031936614535  Gradnorm:0.007563370745629072\n",
      "Iteration: 8.0  Loss: 0.00023727505252730784  Gradnorm:0.0035604117438197136\n",
      "Iteration: 9.0  Loss: 0.0002218866672342778  Gradnorm:0.007257960736751556\n",
      "Iteration: 10.0  Loss: 0.00021083068608269788  Gradnorm:0.0015693921595811844\n",
      "Iteration: 11.0  Loss: 0.00020382682382167748  Gradnorm:0.0017238575965166092\n",
      "Iteration: 12.0  Loss: 0.00019889793292817947  Gradnorm:0.0016991322627291083\n",
      "Iteration: 13.0  Loss: 0.00019932198272872644  Gradnorm:0.002504534088075161\n",
      "Iteration: 14.0  Loss: 0.00019799137105376831  Gradnorm:0.005735066719353199\n",
      "Iteration: 15.0  Loss: 0.00019516661923526122  Gradnorm:0.0008018306689336896\n",
      "Iteration: 16.0  Loss: 0.00019417809853695855  Gradnorm:0.0015163201605901122\n",
      "Iteration: 17.0  Loss: 0.0001926179909199152  Gradnorm:0.0005754430312663317\n",
      "Iteration: 18.0  Loss: 0.0001915039143717554  Gradnorm:0.00222380505874753\n",
      "Iteration: 19.0  Loss: 0.00019448764373193155  Gradnorm:0.010471200570464134\n",
      "Iteration: 20.0  Loss: 0.00019057351266564435  Gradnorm:0.004276530351489782\n",
      "Iteration: 21.0  Loss: 0.0001904081642491461  Gradnorm:0.0021189036779105663\n",
      "Iteration: 22.0  Loss: 0.00019099242528910782  Gradnorm:0.003148474497720599\n",
      "Iteration: 23.0  Loss: 0.00019061214068997608  Gradnorm:0.004942044615745544\n",
      "Iteration: 24.0  Loss: 0.00018962071326094512  Gradnorm:0.0033824287820607424\n",
      "Iteration: 25.0  Loss: 0.00018878524073775627  Gradnorm:0.0012390248011797667\n",
      "Iteration: 26.0  Loss: 0.00018853801345925742  Gradnorm:0.003939216025173664\n",
      "Iteration: 27.0  Loss: 0.00018995473542795862  Gradnorm:0.00584452087059617\n",
      "Iteration: 28.0  Loss: 0.00018935066310272748  Gradnorm:0.005346360616385937\n",
      "Iteration: 29.0  Loss: 0.00018790268640064882  Gradnorm:0.004383281338959932\n",
      "Iteration: 30.0  Loss: 0.0001874868132393668  Gradnorm:0.0024135010316967964\n",
      "Iteration: 31.0  Loss: 0.00018836517420224583  Gradnorm:0.00740963127464056\n",
      "Iteration: 32.0  Loss: 0.00018769622027601482  Gradnorm:0.0013945207465440035\n",
      "Iteration: 33.0  Loss: 0.00018705453758320183  Gradnorm:0.0018698706990107894\n",
      "Iteration: 34.0  Loss: 0.0001870525837292266  Gradnorm:0.0011934505309909582\n",
      "Iteration: 35.0  Loss: 0.00018730698379196243  Gradnorm:0.003484445856884122\n",
      "Iteration: 36.0  Loss: 0.00018677104064225962  Gradnorm:0.0006471544620580971\n",
      "Iteration: 37.0  Loss: 0.00020254757152537002  Gradnorm:0.021905474364757538\n",
      "Iteration: 38.0  Loss: 0.00018634968411559736  Gradnorm:0.0010226103477180004\n",
      "Iteration: 39.0  Loss: 0.00018672884968873663  Gradnorm:0.003265225328505039\n",
      "Iteration: 40.0  Loss: 0.00018695827224677282  Gradnorm:0.0024711766745895147\n",
      "Iteration: 41.0  Loss: 0.00018712325136443147  Gradnorm:0.005446906201541424\n",
      "Iteration: 42.0  Loss: 0.00018615074465722293  Gradnorm:0.003111976431682706\n",
      "Iteration: 43.0  Loss: 0.00018583573826772077  Gradnorm:0.004074679221957922\n",
      "Iteration: 44.0  Loss: 0.0001861745222930782  Gradnorm:0.004069158341735601\n",
      "Iteration: 45.0  Loss: 0.00018986522506534275  Gradnorm:0.011240016669034958\n",
      "Iteration: 46.0  Loss: 0.0001885530576885132  Gradnorm:0.008375509642064571\n",
      "Iteration: 47.0  Loss: 0.00019192367571301968  Gradnorm:0.013877220451831818\n",
      "Iteration: 48.0  Loss: 0.00018539762545224903  Gradnorm:0.0012646070681512356\n",
      "Iteration: 49.0  Loss: 0.00018716974560674689  Gradnorm:0.0016620486276224256\n",
      "Iteration: 50.0  Loss: 0.0001864401528462897  Gradnorm:0.0014007013523951173\n",
      "Iteration: 51.0  Loss: 0.00018519838144566647  Gradnorm:0.000545404152944684\n",
      "Iteration: 52.0  Loss: 0.00018542340342745352  Gradnorm:0.0007795083220116794\n",
      "Iteration: 53.0  Loss: 0.00018623714008866735  Gradnorm:0.0045916675589978695\n",
      "Iteration: 54.0  Loss: 0.00018515372347840183  Gradnorm:0.000619024911429733\n",
      "Iteration: 55.0  Loss: 0.00018646246626627817  Gradnorm:0.007151434198021889\n",
      "Iteration: 56.0  Loss: 0.00018456030837297628  Gradnorm:0.0017497128574177623\n",
      "Iteration: 57.0  Loss: 0.0001850209120633417  Gradnorm:0.003029430750757456\n",
      "Iteration: 58.0  Loss: 0.00018474153364431064  Gradnorm:0.0012712276075035334\n",
      "Iteration: 59.0  Loss: 0.00018500675455066998  Gradnorm:0.0007822768529877067\n",
      "Iteration: 60.0  Loss: 0.0001868196054952445  Gradnorm:0.007743198890239\n",
      "Iteration: 61.0  Loss: 0.00018750529486087527  Gradnorm:0.00337511976249516\n",
      "Iteration: 62.0  Loss: 0.0001864326209823604  Gradnorm:0.006852239836007357\n",
      "Iteration: 63.0  Loss: 0.00018436345112459078  Gradnorm:0.0005951534258201718\n",
      "Iteration: 64.0  Loss: 0.00018499821266498585  Gradnorm:0.00327080930583179\n",
      "Iteration: 65.0  Loss: 0.000185765718491215  Gradnorm:0.004983142949640751\n",
      "Iteration: 66.0  Loss: 0.00018598062915534567  Gradnorm:0.0068393005058169365\n",
      "Iteration: 67.0  Loss: 0.00018700110439305687  Gradnorm:0.009127815254032612\n",
      "Iteration: 68.0  Loss: 0.00018527111972090613  Gradnorm:0.0031819434370845556\n",
      "Iteration: 69.0  Loss: 0.00018568632537805939  Gradnorm:0.002758955815806985\n",
      "Iteration: 70.0  Loss: 0.00018780403251473544  Gradnorm:0.008767882362008095\n",
      "Iteration: 71.0  Loss: 0.0001843870183282351  Gradnorm:0.002014114521443844\n",
      "Iteration: 72.0  Loss: 0.00018552795596374957  Gradnorm:0.004381711594760418\n",
      "Iteration: 73.0  Loss: 0.00018492610134030488  Gradnorm:0.0027083263266831636\n",
      "Iteration: 74.0  Loss: 0.00018495395556432901  Gradnorm:0.0017498814268037677\n",
      "Iteration: 75.0  Loss: 0.00018544905331093262  Gradnorm:0.0008612358360551298\n",
      "Iteration: 76.0  Loss: 0.00018545802965720296  Gradnorm:0.002326673362404108\n",
      "Iteration: 77.0  Loss: 0.00018562062322064453  Gradnorm:0.005905406083911657\n",
      "Iteration: 78.0  Loss: 0.00018487760176652806  Gradnorm:0.0030567168723791838\n",
      "Iteration: 79.0  Loss: 0.0001853146199292609  Gradnorm:0.0044534048065543175\n",
      "Iteration: 80.0  Loss: 0.0001844613548722362  Gradnorm:0.0009599367622286081\n",
      "Iteration: 81.0  Loss: 0.00018486053787551243  Gradnorm:0.0016290928469970822\n",
      "Iteration: 82.0  Loss: 0.0001847319420348972  Gradnorm:0.002900088904425502\n",
      "Iteration: 83.0  Loss: 0.0001849050628470356  Gradnorm:0.0016343662282451987\n",
      "Iteration: 84.0  Loss: 0.00018512909269057556  Gradnorm:0.0025134815368801355\n",
      "Iteration: 85.0  Loss: 0.00018538487407075067  Gradnorm:0.0012079485459253192\n",
      "Iteration: 86.0  Loss: 0.00018473080756272237  Gradnorm:0.0023948082234710455\n",
      "Iteration: 87.0  Loss: 0.00018498411162191247  Gradnorm:0.002457201946526766\n",
      "Iteration: 88.0  Loss: 0.0001861306565152853  Gradnorm:0.0018414044752717018\n",
      "Iteration: 89.0  Loss: 0.00018778687077832115  Gradnorm:0.00846947729587555\n",
      "Iteration: 90.0  Loss: 0.00018461043176338273  Gradnorm:0.001769687863998115\n",
      "Iteration: 91.0  Loss: 0.00018477118059454818  Gradnorm:0.0025054416619241238\n",
      "Iteration: 92.0  Loss: 0.0001854196526480176  Gradnorm:0.006236115004867315\n",
      "Iteration: 93.0  Loss: 0.00018475659229196098  Gradnorm:0.0029875761829316616\n",
      "Iteration: 94.0  Loss: 0.0001851536172345086  Gradnorm:0.002618820872157812\n",
      "Iteration: 95.0  Loss: 0.0001854159240394328  Gradnorm:0.0008004053961485624\n",
      "Iteration: 96.0  Loss: 0.00018615646914162134  Gradnorm:0.004041416570544243\n",
      "Iteration: 97.0  Loss: 0.00018487479035577697  Gradnorm:0.0032403869554400444\n",
      "Iteration: 98.0  Loss: 0.00018486117325393106  Gradnorm:0.0012270929291844368\n",
      "Iteration: 99.0  Loss: 0.00018691476502009566  Gradnorm:0.007659208029508591\n",
      "Iteration: 100.0  Loss: 0.00018433716859367206  Gradnorm:0.0015345915453508496\n"
     ]
    }
   ],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-adam \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 1e-4\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/m)\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "\n",
    "onr_gadam_loss_list = []\n",
    "onr_gadam_gradnorm_list = []\n",
    "onr_gadam_test_loss_list = []\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "batch_idx = torch.tensor(0)\n",
    "h_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = LR_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2)  in enumerate(zip(list(LR_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_gadam_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_gadam_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_gadam_test_loss_list.append(current_test_loss)\n",
    "            current_iteration =  (epoch+1)/max_batch_size\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72afbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Loss: 0.00033541151059457566  Gradnorm:0.04113851860165596\n",
      "Iteration: 2  Loss: 0.0002160087850367104  Gradnorm:0.008576749823987484\n",
      "Iteration: 3  Loss: 0.00020230722135124046  Gradnorm:0.0047256723046302795\n",
      "Iteration: 4  Loss: 0.0002004726707521869  Gradnorm:0.010763927362859249\n",
      "Iteration: 5  Loss: 0.00020953042562754396  Gradnorm:0.02246834523975849\n",
      "Iteration: 6  Loss: 0.000191614780725975  Gradnorm:0.0015589187387377024\n",
      "Iteration: 7  Loss: 0.00019081645920795165  Gradnorm:0.0032060574740171432\n",
      "Iteration: 8  Loss: 0.00018906331568693262  Gradnorm:0.002086319262161851\n",
      "Iteration: 9  Loss: 0.00019015422410676857  Gradnorm:0.006270367186516523\n",
      "Iteration: 10  Loss: 0.00018820835627766979  Gradnorm:0.0005005186540074646\n",
      "Iteration: 11  Loss: 0.00018853894912790618  Gradnorm:0.0017259467858821154\n",
      "Iteration: 12  Loss: 0.0002089522404873125  Gradnorm:0.02331104874610901\n",
      "Iteration: 13  Loss: 0.00019202047441258674  Gradnorm:0.010758776217699051\n",
      "Iteration: 14  Loss: 0.00018813946736139886  Gradnorm:0.0012875179527327418\n",
      "Iteration: 15  Loss: 0.00018772859171552086  Gradnorm:0.0011697531444951892\n",
      "Iteration: 16  Loss: 0.00019478450432224793  Gradnorm:0.0036495872773230076\n",
      "Iteration: 17  Loss: 0.00018702006129300993  Gradnorm:0.00221907626837492\n",
      "Iteration: 18  Loss: 0.0001874335352451798  Gradnorm:0.0012283886317163706\n",
      "Iteration: 19  Loss: 0.00020515446795023486  Gradnorm:0.023656511679291725\n",
      "Iteration: 20  Loss: 0.00018701169148836906  Gradnorm:0.003200202714651823\n",
      "Iteration: 21  Loss: 0.0001857664896844502  Gradnorm:0.00038553643389604986\n",
      "Iteration: 22  Loss: 0.00018641422035053066  Gradnorm:0.0007907733088359237\n",
      "Iteration: 23  Loss: 0.00018632760406217045  Gradnorm:0.003994693513959646\n",
      "Iteration: 24  Loss: 0.00018617521440538346  Gradnorm:0.0036828459706157446\n",
      "Iteration: 25  Loss: 0.00018602178618370614  Gradnorm:0.0037845070473849773\n",
      "Iteration: 26  Loss: 0.00018559048939264294  Gradnorm:0.0020146756432950497\n",
      "Iteration: 27  Loss: 0.0001853531735711212  Gradnorm:0.001020643743686378\n",
      "Iteration: 28  Loss: 0.0001867507625064056  Gradnorm:0.006557558197528124\n",
      "Iteration: 29  Loss: 0.00018572227736597612  Gradnorm:0.003717588959261775\n",
      "Iteration: 30  Loss: 0.0001860191135243082  Gradnorm:0.004033470991998911\n",
      "Iteration: 31  Loss: 0.00018489423704066526  Gradnorm:0.0004592793993651867\n",
      "Iteration: 32  Loss: 0.00018494767939280433  Gradnorm:0.001004969934001565\n",
      "Iteration: 33  Loss: 0.00018565245850730278  Gradnorm:0.004904361441731453\n",
      "Iteration: 34  Loss: 0.0001849026238184875  Gradnorm:0.0005243566120043397\n",
      "Iteration: 35  Loss: 0.0001853289594484695  Gradnorm:0.002244706032797694\n",
      "Iteration: 36  Loss: 0.00018521206520689262  Gradnorm:0.0008032252080738544\n",
      "Iteration: 37  Loss: 0.0001852177539830063  Gradnorm:0.0016024536453187466\n",
      "Iteration: 38  Loss: 0.00018533746265996694  Gradnorm:0.0014050048775970936\n",
      "Iteration: 39  Loss: 0.00018521968872012843  Gradnorm:0.001660373993217945\n",
      "Iteration: 40  Loss: 0.0002006213067810975  Gradnorm:0.004147172439843416\n",
      "Iteration: 41  Loss: 0.00018547566382362137  Gradnorm:0.004754137713462114\n",
      "Iteration: 42  Loss: 0.0001980938506008003  Gradnorm:0.02013366110622883\n",
      "Iteration: 43  Loss: 0.00018905325349753314  Gradnorm:0.0028570247814059258\n",
      "Iteration: 44  Loss: 0.00018563869948264424  Gradnorm:0.002960505895316601\n",
      "Iteration: 45  Loss: 0.00018511630169519883  Gradnorm:0.0007987807621248066\n",
      "Iteration: 46  Loss: 0.00018620009277260833  Gradnorm:0.006595335900783539\n",
      "Iteration: 47  Loss: 0.0001848622554263895  Gradnorm:0.000988587155006826\n",
      "Iteration: 48  Loss: 0.00018481287818056128  Gradnorm:0.001593486755155027\n",
      "Iteration: 49  Loss: 0.0001846160327370528  Gradnorm:0.0002787720295600593\n",
      "Iteration: 50  Loss: 0.00018464249627608598  Gradnorm:0.001489010639488697\n",
      "Iteration: 51  Loss: 0.00018832944154424585  Gradnorm:0.005464760586619377\n",
      "Iteration: 52  Loss: 0.00018496434544184486  Gradnorm:0.0021280560176819563\n",
      "Iteration: 53  Loss: 0.00018460189387138834  Gradnorm:0.00034395031980238855\n",
      "Iteration: 54  Loss: 0.00018483043884546925  Gradnorm:0.001621830277144909\n",
      "Iteration: 55  Loss: 0.00018455428574201945  Gradnorm:0.00033008414902724326\n",
      "Iteration: 56  Loss: 0.00018456736601561418  Gradnorm:0.0008774046436883509\n",
      "Iteration: 57  Loss: 0.0001845177889091067  Gradnorm:0.0003121910849586129\n",
      "Iteration: 58  Loss: 0.0001847185524559051  Gradnorm:0.0027626603841781616\n",
      "Iteration: 59  Loss: 0.00018447966429523992  Gradnorm:0.0013365232152864337\n",
      "Iteration: 60  Loss: 0.0001847970197378095  Gradnorm:0.002113603288307786\n",
      "Iteration: 61  Loss: 0.00018467669864614624  Gradnorm:0.0023311441764235497\n",
      "Iteration: 62  Loss: 0.00018458613612247572  Gradnorm:0.0011653255205601454\n",
      "Iteration: 63  Loss: 0.00018528573242611226  Gradnorm:0.004425517283380032\n",
      "Iteration: 64  Loss: 0.00018455457220116232  Gradnorm:0.0009046081686392426\n",
      "Iteration: 65  Loss: 0.00018472381193973693  Gradnorm:0.002769757993519306\n",
      "Iteration: 66  Loss: 0.0001847569020084735  Gradnorm:0.00253808475099504\n",
      "Iteration: 67  Loss: 0.00018469181763962317  Gradnorm:0.0012125001521781087\n",
      "Iteration: 68  Loss: 0.00018455986263608964  Gradnorm:0.0010599150555208325\n",
      "Iteration: 69  Loss: 0.00018457708125321738  Gradnorm:0.0003804427105933428\n",
      "Iteration: 70  Loss: 0.00018530821936604694  Gradnorm:0.003861451055854559\n",
      "Iteration: 71  Loss: 0.00018468401831594266  Gradnorm:0.001667427597567439\n",
      "Iteration: 72  Loss: 0.00018479864337824693  Gradnorm:0.0013708563055843115\n",
      "Iteration: 73  Loss: 0.0001846837652158553  Gradnorm:0.001615673303604126\n",
      "Iteration: 74  Loss: 0.00018474547679625758  Gradnorm:0.0005744592635892332\n",
      "Iteration: 75  Loss: 0.00018481018387888877  Gradnorm:0.0018371097976341844\n",
      "Iteration: 76  Loss: 0.00018463180581468637  Gradnorm:0.00034632848110049963\n",
      "Iteration: 77  Loss: 0.00018489954359717837  Gradnorm:0.0007660420960746706\n",
      "Iteration: 78  Loss: 0.00018476594263552943  Gradnorm:0.0011526347370818257\n",
      "Iteration: 79  Loss: 0.00018484375862298696  Gradnorm:0.0017638035351410508\n",
      "Iteration: 80  Loss: 0.00018484908185902783  Gradnorm:0.0011131762294098735\n",
      "Iteration: 81  Loss: 0.00018475882303196984  Gradnorm:0.002149363746866584\n",
      "Iteration: 82  Loss: 0.0001847152440069734  Gradnorm:0.002003354486078024\n",
      "Iteration: 83  Loss: 0.0001845646061998107  Gradnorm:0.0003305738209746778\n",
      "Iteration: 84  Loss: 0.00018492860121389077  Gradnorm:0.002800517715513706\n",
      "Iteration: 85  Loss: 0.0001848766196418707  Gradnorm:0.0028423548210412264\n",
      "Iteration: 86  Loss: 0.00018546569903914005  Gradnorm:0.004935997072607279\n",
      "Iteration: 87  Loss: 0.00018485069751208576  Gradnorm:0.003114864928647876\n",
      "Iteration: 88  Loss: 0.00018465714651022953  Gradnorm:0.0012334836646914482\n",
      "Iteration: 89  Loss: 0.00018455414293824584  Gradnorm:0.0005264196661300957\n",
      "Iteration: 90  Loss: 0.00018449779514745455  Gradnorm:0.0003761320258490741\n",
      "Iteration: 91  Loss: 0.0001846696291399006  Gradnorm:0.002612659242004156\n",
      "Iteration: 92  Loss: 0.00018445148382200235  Gradnorm:0.0008328324183821678\n",
      "Iteration: 93  Loss: 0.00018463139478766202  Gradnorm:0.0022371718659996986\n",
      "Iteration: 94  Loss: 0.00018440947352420942  Gradnorm:0.00021823091083206236\n",
      "Iteration: 95  Loss: 0.00018448146471517914  Gradnorm:0.0013513885205611587\n",
      "Iteration: 96  Loss: 0.00018458600165846596  Gradnorm:0.002281673951074481\n",
      "Iteration: 97  Loss: 0.00018470419581668828  Gradnorm:0.003062062431126833\n",
      "Iteration: 98  Loss: 0.00018486748348928  Gradnorm:0.0010903641814365983\n",
      "Iteration: 99  Loss: 0.00018443442257386962  Gradnorm:0.001057358575053513\n",
      "Iteration: 100  Loss: 0.00018514881775913116  Gradnorm:0.004894237965345383\n"
     ]
    }
   ],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-as \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 0.1\n",
    "lr_schedule = 't-inverse'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/m)\n",
    "onr_ggdas_loss_list = []\n",
    "onr_ggdas_gradnorm_list = []\n",
    "onr_ggdas_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate loss for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(epoch/(5*max_batch_size)+1))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = LR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(LR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)    \n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_ggdas_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_ggdas_test_loss_list.append(current_test_loss)\n",
    "        current_iteration =  int((epoch+1)/max_batch_size)\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e32288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1.0  Loss: 0.009064189947574236  Gradnorm:0.5098594427108765\n",
      "Iteration: 2.0  Loss: 0.00036832878788093016  Gradnorm:0.02865799330174923\n",
      "Iteration: 3.0  Loss: 0.00022537484518383725  Gradnorm:0.0045554074458777905\n",
      "Iteration: 4.0  Loss: 0.00020851223489782629  Gradnorm:0.0018601295305415988\n",
      "Iteration: 5.0  Loss: 0.0002004540025431445  Gradnorm:0.0010745780309662223\n",
      "Iteration: 6.0  Loss: 0.00019547760221727184  Gradnorm:0.0007609976455569267\n",
      "Iteration: 7.0  Loss: 0.0001922941586522768  Gradnorm:0.0014818457420915365\n",
      "Iteration: 8.0  Loss: 0.00019003919024920777  Gradnorm:0.0003607641556300223\n",
      "Iteration: 9.0  Loss: 0.00018852756308908815  Gradnorm:0.00044145958963781595\n",
      "Iteration: 10.0  Loss: 0.00018744819696118136  Gradnorm:0.0004522191593423486\n",
      "Iteration: 11.0  Loss: 0.00018665942597042475  Gradnorm:0.00020035200577694923\n",
      "Iteration: 12.0  Loss: 0.00018608549566613225  Gradnorm:0.000460511859273538\n",
      "Iteration: 13.0  Loss: 0.0001856443662347336  Gradnorm:0.00018309729057364166\n",
      "Iteration: 14.0  Loss: 0.00018531412016010128  Gradnorm:5.872076144441962e-05\n",
      "Iteration: 15.0  Loss: 0.00018506006173403053  Gradnorm:0.00016825996863190085\n",
      "Iteration: 16.0  Loss: 0.0001848649914856121  Gradnorm:8.051068289205432e-05\n",
      "Iteration: 17.0  Loss: 0.0001847105026456548  Gradnorm:5.6324955949094146e-05\n",
      "Iteration: 18.0  Loss: 0.00018458854167450683  Gradnorm:6.083750486141071e-05\n",
      "Iteration: 19.0  Loss: 0.00018449085282260793  Gradnorm:0.0001987788564292714\n",
      "Iteration: 20.0  Loss: 0.00018440806466205704  Gradnorm:3.523381383274682e-05\n",
      "Iteration: 21.0  Loss: 0.0001843410405639724  Gradnorm:5.76611164433416e-05\n",
      "Iteration: 22.0  Loss: 0.00018428463013685597  Gradnorm:2.871770630008541e-05\n",
      "Iteration: 23.0  Loss: 0.0001842362812367708  Gradnorm:3.7378213164629415e-05\n",
      "Iteration: 24.0  Loss: 0.00018419460244459176  Gradnorm:0.00010342190944356844\n",
      "Iteration: 25.0  Loss: 0.0001841573729164383  Gradnorm:5.837566641275771e-05\n",
      "Iteration: 26.0  Loss: 0.0001841250692521964  Gradnorm:4.267567055649124e-05\n",
      "Iteration: 27.0  Loss: 0.00018409569927608856  Gradnorm:3.428847048780881e-05\n",
      "Iteration: 28.0  Loss: 0.00018406922172977593  Gradnorm:4.6856650442350656e-05\n",
      "Iteration: 29.0  Loss: 0.00018404491971658454  Gradnorm:1.7903912521433085e-05\n",
      "Iteration: 30.0  Loss: 0.00018402278889044027  Gradnorm:6.321220280369744e-05\n",
      "Iteration: 31.0  Loss: 0.00018400213995811297  Gradnorm:4.7805922804400325e-05\n",
      "Iteration: 32.0  Loss: 0.00018398278829954824  Gradnorm:6.58900971757248e-05\n",
      "Iteration: 33.0  Loss: 0.00018396442998321055  Gradnorm:1.8121349057764746e-05\n",
      "Iteration: 34.0  Loss: 0.00018394717580462488  Gradnorm:2.660671634657774e-05\n",
      "Iteration: 35.0  Loss: 0.00018393081729905982  Gradnorm:4.1925362893380225e-05\n",
      "Iteration: 36.0  Loss: 0.00018391550414178457  Gradnorm:1.2483638784033246e-05\n",
      "Iteration: 37.0  Loss: 0.00018390066811297193  Gradnorm:2.7792906621471047e-05\n",
      "Iteration: 38.0  Loss: 0.0001838865770541229  Gradnorm:1.644125768507365e-05\n",
      "Iteration: 39.0  Loss: 0.00018387316374791527  Gradnorm:2.5552195438649505e-05\n",
      "Iteration: 40.0  Loss: 0.00018386009146170005  Gradnorm:1.1420177543186583e-05\n",
      "Iteration: 41.0  Loss: 0.00018384787168141778  Gradnorm:1.1654239642666653e-05\n",
      "Iteration: 42.0  Loss: 0.00018383577411978775  Gradnorm:1.1234368685109075e-05\n",
      "Iteration: 43.0  Loss: 0.0001838241126632044  Gradnorm:1.6168343790923245e-05\n",
      "Iteration: 44.0  Loss: 0.00018381274888333362  Gradnorm:1.0799959454743657e-05\n",
      "Iteration: 45.0  Loss: 0.00018380174758953757  Gradnorm:1.5451132640009746e-05\n",
      "Iteration: 46.0  Loss: 0.00018379122089878273  Gradnorm:1.7530415789224207e-05\n",
      "Iteration: 47.0  Loss: 0.0001837810032197716  Gradnorm:1.0138584002561402e-05\n",
      "Iteration: 48.0  Loss: 0.0001837709970301246  Gradnorm:2.3168513507698663e-05\n",
      "Iteration: 49.0  Loss: 0.00018376127489166044  Gradnorm:1.0010066034737974e-05\n",
      "Iteration: 50.0  Loss: 0.00018375181671846896  Gradnorm:2.1436819224618375e-05\n",
      "Iteration: 51.0  Loss: 0.00018374266752765588  Gradnorm:9.7363990789745e-06\n",
      "Iteration: 52.0  Loss: 0.00018373384464478692  Gradnorm:3.1727693567518145e-05\n",
      "Iteration: 53.0  Loss: 0.00018372515719498482  Gradnorm:9.363881872559432e-06\n",
      "Iteration: 54.0  Loss: 0.00018371678060694448  Gradnorm:1.3495267012331169e-05\n",
      "Iteration: 55.0  Loss: 0.00018370859691999172  Gradnorm:1.2146897461207118e-05\n",
      "Iteration: 56.0  Loss: 0.00018370059318400023  Gradnorm:1.0512236258364283e-05\n",
      "Iteration: 57.0  Loss: 0.00018369281332955722  Gradnorm:8.870182682585437e-06\n",
      "Iteration: 58.0  Loss: 0.00018368528349183813  Gradnorm:1.2128377420594916e-05\n",
      "Iteration: 59.0  Loss: 0.00018367789393247124  Gradnorm:1.0964689863612875e-05\n",
      "Iteration: 60.0  Loss: 0.0001836706953360778  Gradnorm:8.787881597527303e-06\n",
      "Iteration: 61.0  Loss: 0.00018366368435500605  Gradnorm:8.829081707517616e-06\n",
      "Iteration: 62.0  Loss: 0.00018365684712880347  Gradnorm:8.623377652838826e-06\n",
      "Iteration: 63.0  Loss: 0.00018365015881437055  Gradnorm:8.194269867090043e-06\n",
      "Iteration: 64.0  Loss: 0.0001836436897417583  Gradnorm:9.584324288880453e-06\n",
      "Iteration: 65.0  Loss: 0.00018363734277033683  Gradnorm:9.767441042640712e-06\n",
      "Iteration: 66.0  Loss: 0.00018363109346812182  Gradnorm:1.3658130228577647e-05\n",
      "Iteration: 67.0  Loss: 0.00018362504029718462  Gradnorm:9.905458682624158e-06\n",
      "Iteration: 68.0  Loss: 0.0001836190555475496  Gradnorm:1.5661025827284902e-05\n",
      "Iteration: 69.0  Loss: 0.00018361328886512063  Gradnorm:1.0229508916381747e-05\n",
      "Iteration: 70.0  Loss: 0.0001836075961834133  Gradnorm:9.998343557526823e-06\n",
      "Iteration: 71.0  Loss: 0.00018360207106048296  Gradnorm:8.266737495432608e-06\n",
      "Iteration: 72.0  Loss: 0.00018359666219503558  Gradnorm:7.403411473205779e-06\n",
      "Iteration: 73.0  Loss: 0.00018359135408215808  Gradnorm:7.295992872968782e-06\n",
      "Iteration: 74.0  Loss: 0.00018358619000639954  Gradnorm:8.506753147230484e-06\n",
      "Iteration: 75.0  Loss: 0.0001835811169045441  Gradnorm:8.020679160836153e-06\n",
      "Iteration: 76.0  Loss: 0.00018357617183568332  Gradnorm:7.412510967697017e-06\n",
      "Iteration: 77.0  Loss: 0.00018357131133907585  Gradnorm:7.1420854510506615e-06\n",
      "Iteration: 78.0  Loss: 0.00018356657902228988  Gradnorm:7.320719760173233e-06\n",
      "Iteration: 79.0  Loss: 0.00018356193171823775  Gradnorm:6.976427357585635e-06\n",
      "Iteration: 80.0  Loss: 0.00018355739958515006  Gradnorm:7.536924385931343e-06\n",
      "Iteration: 81.0  Loss: 0.00018355294823618355  Gradnorm:1.1991366591246333e-05\n",
      "Iteration: 82.0  Loss: 0.0001835485986675747  Gradnorm:7.1645031312073115e-06\n",
      "Iteration: 83.0  Loss: 0.00018354436356516154  Gradnorm:9.522833352093585e-06\n",
      "Iteration: 84.0  Loss: 0.00018354017603464088  Gradnorm:6.85517443343997e-06\n",
      "Iteration: 85.0  Loss: 0.00018353609257497642  Gradnorm:6.936947556823725e-06\n",
      "Iteration: 86.0  Loss: 0.0001835320842612829  Gradnorm:6.823929197707912e-06\n",
      "Iteration: 87.0  Loss: 0.00018352817276520025  Gradnorm:9.377682545164134e-06\n",
      "Iteration: 88.0  Loss: 0.00018352433623889636  Gradnorm:7.408001238218276e-06\n",
      "Iteration: 89.0  Loss: 0.0001835206035778911  Gradnorm:6.62911998006166e-06\n",
      "Iteration: 90.0  Loss: 0.00018351692248246807  Gradnorm:6.437283445848152e-06\n",
      "Iteration: 91.0  Loss: 0.00018351331941082185  Gradnorm:6.178131570777623e-06\n",
      "Iteration: 92.0  Loss: 0.00018350979186689632  Gradnorm:6.6868287831312045e-06\n",
      "Iteration: 93.0  Loss: 0.0001835063188250896  Gradnorm:5.998300821374869e-06\n",
      "Iteration: 94.0  Loss: 0.00018350293567066743  Gradnorm:5.968998266325798e-06\n",
      "Iteration: 95.0  Loss: 0.00018349961133507287  Gradnorm:5.785538178315619e-06\n",
      "Iteration: 96.0  Loss: 0.00018349636848836824  Gradnorm:6.528260655613849e-06\n",
      "Iteration: 97.0  Loss: 0.00018349315961739177  Gradnorm:5.6838330237951595e-06\n",
      "Iteration: 98.0  Loss: 0.00018349002692017406  Gradnorm:8.71895463205874e-06\n",
      "Iteration: 99.0  Loss: 0.0001834869481671332  Gradnorm:5.7245351854362525e-06\n",
      "Iteration: 100.0  Loss: 0.00018348395883163105  Gradnorm:6.657304311374901e-06\n"
     ]
    }
   ],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-svrg\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "Snap_model = copy.deepcopy(LR_net)\n",
    "LR_net.to(device)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 0.1\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/m)\n",
    "q = max_batch_size\n",
    "onr_gsvrg_loss_list = []\n",
    "onr_gsvrg_gradnorm_list = []\n",
    "onr_gsvrg_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set and testing set\n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "fg = full_grad(Snap_model, loss_func_rec, onr_train_loader, n)\n",
    "batch_idx = 0\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #part one: prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = LR_net(x)\n",
    "            loss = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(LR_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(LR_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equal update frequency\n",
    "    if  batch_idx  % q == 0:\n",
    "        Snap_model = copy.deepcopy(LR_net)\n",
    "        fg = full_grad(Snap_model, loss_func_rec, onr_train_loader, n)\n",
    "    if batch_idx % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_gsvrg_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_gsvrg_test_loss_list.append(current_test_loss)\n",
    "            current_iteration = batch_idx / max_batch_size\n",
    "            print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
