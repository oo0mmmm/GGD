{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, RandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch import linalg as la\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        return x  \n",
    "    \n",
    "\n",
    "#Reads Onr dataset\n",
    "class CSVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = dataset.iloc[:,:-1].values\n",
    "        self.labels = dataset.iloc[:,-1].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        feature = torch.tensor(feature, dtype = torch.float)\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype = torch.float)\n",
    "        return feature, label\n",
    "    \n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "            \n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for x, y in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    " \n",
    "    \n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for x, y in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y.float().view(x.shape[0], -1))\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad \n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "#Reads Onr dataset, scales and transforms it into class of 'torch.dataset'\n",
    "onr_data = pd.read_csv('OnlineNewsPopularity.csv', dtype = np.float32)\n",
    "MaxminScaler = preprocessing.MinMaxScaler()\n",
    "onr_data = MaxminScaler.fit_transform(onr_data)\n",
    "onr_data = pd.DataFrame(onr_data)\n",
    "onr_dataset = CSVDataset(onr_data)     \n",
    "d = 58\n",
    "nt = 39644\n",
    "n = 31715\n",
    "n1 = nt - n\n",
    "epoches = 100\n",
    "weight_decay = 1e-6\n",
    "rbs = 500\n",
    "#Loss function that is used to record the training, testing losses and l2-norm of full gradients\n",
    "loss_func_rec = nn.MSELoss(reduction='sum')\n",
    "#Split the Onr dataset into training set and testing set.\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "onr_train, onr_test = torch.utils.data.random_split(onr_dataset, [n,n1], generator = generator1)\n",
    "#Dataloaders which are used to calculate the training, testing losses and l2-norm of full gradients\n",
    "onr_train_loader = DataLoader(onr_train, batch_size = rbs)\n",
    "onr_train_loader = DeviceDataLoader(onr_train_loader, device)\n",
    "onr_test_loader = DataLoader(onr_test, batch_size = rbs)\n",
    "onr_test_loader = DeviceDataLoader(onr_test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f546c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-adam \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 1e-4\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/(b*m))\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "\n",
    "onr_gadam_loss_list = []\n",
    "onr_gadam_gradnorm_list = []\n",
    "onr_gadam_test_loss_list = []\n",
    "\n",
    "#stage two: load training set \n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "batch_idx = torch.tensor(0)\n",
    "h_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(LR_net.parameters())]\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = LR_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2)  in enumerate(zip(list(LR_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_gadam_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_gadam_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_gadam_test_loss_list.append(current_test_loss)\n",
    "            current_iteration =  (epoch+1)/max_batch_size\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-as \n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "LR_net.to(device)\n",
    "copied_model = copy.deepcopy(LR_net)\n",
    "copied_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 0.1\n",
    "lr_schedule = 't-inverse'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/(b*m))\n",
    "onr_ggdas_loss_list = []\n",
    "onr_ggdas_gradnorm_list = []\n",
    "onr_ggdas_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = LR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i].float().view(x.shape[0], -1)).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(LR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(epoch/(5*max_batch_size)+1))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = LR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0].float().view(xt[0].shape[0], -1))\n",
    "        loss2 = loss_func(output2, yt[1].float().view(xt[1].shape[0], -1))\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(LR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(LR_net)\n",
    "        copied_model.to(device)    \n",
    "    if (epoch+1) % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_ggdas_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_ggdas_test_loss_list.append(current_test_loss)\n",
    "        current_iteration =  int((epoch+1)/max_batch_size)\n",
    "        print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: Linear regression with l^2 regularization using ggd-svrg\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "device = torch.device(\"cpu\")\n",
    "LR_net = LinearRegression(input_dim = d, output_dim = 1)\n",
    "Snap_model = copy.deepcopy(LR_net)\n",
    "LR_net.to(device)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "lr0 = 0.1\n",
    "lr_schedule = 'constant'\n",
    "b = 2\n",
    "m = 32\n",
    "max_batch_size = int(n/(m))\n",
    "q = max_batch_size\n",
    "onr_gsvrg_loss_list = []\n",
    "onr_gsvrg_gradnorm_list = []\n",
    "onr_gsvrg_test_loss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "BS = BatchSampler(WeightedRandomSampler(torch.ones(n), replacement = False, num_samples = b*m), batch_size = b*m, drop_last = False)\n",
    "LR_onr_train_loader = DataLoader(onr_train, batch_sampler = BS)\n",
    "LR_onr_train_loader = DeviceDataLoader(LR_onr_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "fg = full_grad(Snap_model, loss_func_rec, onr_train_loader, n)\n",
    "batch_idx = 0\n",
    "for epoch in range(epoches*max_batch_size):\n",
    "    LR_net.train()\n",
    "    for x_data, y_target in LR_onr_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 't-inverse':\n",
    "            lr = lr0 * (1/int(1 + epoch + batch_idx/n))\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #part one: prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = LR_net(x)\n",
    "            loss = loss_func(output, yt[i].float().view(x.shape[0], -1))\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(LR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(LR_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(LR_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equal update frequency\n",
    "    if  batch_idx  % q == 0:\n",
    "        Snap_model = copy.deepcopy(LR_net)\n",
    "        fg = full_grad(Snap_model, loss_func_rec, onr_train_loader, n)\n",
    "    if batch_idx % max_batch_size == 0:\n",
    "        LR_net.eval()\n",
    "        current_gradnorm = total_grad(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "        onr_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "        with torch.no_grad():\n",
    "            current_loss = total_loss(LR_net, loss_func_rec, onr_train_loader, n)\n",
    "            onr_gsvrg_loss_list.append(current_loss)\n",
    "            current_test_loss = test_loss(LR_net, loss_func_rec, onr_test_loader, n1)\n",
    "            onr_gsvrg_test_loss_list.append(current_test_loss)\n",
    "            current_iteration = batch_idx / max_batch_size\n",
    "            print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
