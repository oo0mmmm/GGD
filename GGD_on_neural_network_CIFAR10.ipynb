{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import copy\n",
    "import csv\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import linalg as la\n",
    "\n",
    "\n",
    "class CIFAR_net(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super(CIFAR_net, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 320, kernel_size = 3, stride = 1, padding = 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 320, out_channels = 320, kernel_size = 1, stride = 1, padding = 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 320, out_channels = 10, kernel_size = 1, stride = 1, padding = 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        out = self.conv9(x)\n",
    "        out = self.avgpool(out).view(x.shape[0], -1)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#Resets gradients of all model parameters\n",
    "def zero_grad(params):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach()\n",
    "            p.grad.zero_()\n",
    "\n",
    "            \n",
    "#Records the training losses\n",
    "def total_loss(model, loss_fns, dataloader, n):\n",
    "    total_loss = 0\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the testing losses\n",
    "def test_loss(model, loss_fns, test_dataloader, n):\n",
    "    test_loss = 0\n",
    "    for y, x in test_dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        test_loss = test_loss + loss.item()\n",
    "    return test_loss * (1/n)\n",
    "\n",
    "\n",
    "#Records the l2 norm of gradients\n",
    "def total_grad(model, loss_fns, dataloader, n):\n",
    "    total_grad = 0\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        total_grad = total_grad + torch.sum(torch.square(torch.mul(torch.clone(p.grad.data).detach(), (1/n))))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return torch.sqrt(torch.clone(total_grad).detach())\n",
    "\n",
    "\n",
    "#Records the full gradients (used in SVRG and GGD-SVRG algorithms)\n",
    "def full_grad(model, loss_fns, dataloader, n):\n",
    "    full_grad = []\n",
    "    zero_grad(list(model.parameters()))\n",
    "    for y, x in dataloader:\n",
    "        out = model(x)\n",
    "        loss = loss_fns(out, y)\n",
    "        loss.backward()\n",
    "    for p in list(model.parameters()):\n",
    "        full_grad.append(torch.mul(torch.clone(p.grad.data).detach(), (1/n)))\n",
    "    zero_grad(list(model.parameters()))\n",
    "    return full_grad \n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    \n",
    "def save_result(filename, listname, ty):\n",
    "    if ty :\n",
    "        with open(filename, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for th in range(len(listname)):\n",
    "                writer.writerow([listname[th]])\n",
    "    else:\n",
    "        with open(filename, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for th in range(len(listname)):\n",
    "                writer.writerow([float(listname[th].cpu())])\n",
    "\n",
    "    \n",
    "CIFAR_train = torchvision.datasets.CIFAR10(root='.test/', train = True, download = True, transform = ToTensor())\n",
    "CIFAR_test = torchvision.datasets.CIFAR10(root='.test/', train = False, download = True, transform = ToTensor())\n",
    "n = len(CIFAR_train)\n",
    "n1 = len(CIFAR_test)\n",
    "weight_decay = 1e-4\n",
    "nabla = 1e-4\n",
    "rbs = 500\n",
    "epoches = 150\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Loss function that is used to record the experiment results.\n",
    "loss_func_rec = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "#Dataloaders that are used to calculate the training, testing losses and l2-norm of full gradients.\n",
    "CIFAR_train_loader_eva = DataLoader(CIFAR_train, batch_size = rbs)\n",
    "CIFAR_train_loader_eva = DeviceDataLoader(CIFAR_train_loader_eva, device)\n",
    "CIFAR_test_loader_eva = DataLoader(CIFAR_test, batch_size = rbs)\n",
    "CIFAR_test_loader_eva = DeviceDataLoader(CIFAR_test_loader_eva, device)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f08170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: training cifar-nv using cifar-10 dataset with ggd-as\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "CNN_CIFAR_net = CIFAR_net()\n",
    "CNN_CIFAR_net = CNN_CIFAR_net.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "copied_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "copied_model.to(device)\n",
    "#initial learning rate\n",
    "ilr0 = 0.01\n",
    "#final learning rate\n",
    "flr0 = 1e-4\n",
    "#learning schedule\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "CIFAR_ggdas_loss_list = []\n",
    "CIFAR_ggdas_gradnorm_list = []\n",
    "CIFAR_ggdas_testloss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set\n",
    "CIFAR_train_loader = DataLoader(CIFAR_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "CIFAR_train_loader = DeviceDataLoader(CIFAR_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "for epoch in range(epoches):\n",
    "    CNN_CIFAR_net.train()\n",
    "    for x_data, y_target in CIFAR_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = CNN_CIFAR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i]).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(CNN_CIFAR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch)\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the grafting gradient\n",
    "        output1 = CNN_CIFAR_net(xt[0])\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss1 = loss_func(output1, yt[0])\n",
    "        loss2 = loss_func(output2, yt[1])\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        for  p1, p2 in zip(list(CNN_CIFAR_net.parameters()), list(copied_model.parameters())):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            p1.data.add_(torch.add(d_p1, d_p2), alpha = -lr)\n",
    "        copied_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "        copied_model.to(device)    \n",
    "    CNN_CIFAR_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "    CIFAR_ggdas_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "        CIFAR_ggdas_loss_list.append(current_loss)\n",
    "        CIFAR_ggdas_test_loss_list.append(test_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_test_loader_eva, n1))\n",
    "        current_iteration =  epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: training cifar-nv using cifar-10 dataset with ggd-adam\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "CNN_CIFAR_net = CIFAR_net()\n",
    "CNN_CIFAR_net = CNN_CIFAR_net.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "copied_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "copied_model.to(device)\n",
    "#initial learning rate\n",
    "ilr0 = 1e-4\n",
    "#final learning rate\n",
    "flr0 = 1e-5\n",
    "#learning schedule\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "beta_1 = torch.tensor(0.9)\n",
    "beta_2 = torch.tensor(0.999)\n",
    "sigma = torch.tensor(1e-8)\n",
    "\n",
    "CIFAR_gadamas_loss_list = []\n",
    "CIFAR_gadamas_gradnorm_list = []\n",
    "CIFAR_gadamas_testloss_list = []\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "CIFAR_train_loader = DataLoader(CIFAR_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "CIFAR_train_loader = DeviceDataLoader(CIFAR_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test \n",
    "batch_idx = torch.tensor(0)\n",
    "h_0 = [torch.zeros_like(paras) for paras in list(CNN_CIFAR_net.parameters())]\n",
    "v_0 = [torch.zeros_like(paras) for paras in list(CNN_CIFAR_net.parameters())]\n",
    "for epoch in range(epoches):\n",
    "    CNN_CIFAR_net.train()\n",
    "    for x_data, y_target in CIFAR_train_loader:\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch)\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #calculate losses for first to derive the resampling probability\n",
    "        for i, x in enumerate(xt):\n",
    "            with torch.no_grad():\n",
    "                output = CNN_CIFAR_net(x)\n",
    "                losst[i] = loss_func(output, yt[i]).item()\n",
    "        prob = losst/torch.sum(losst)\n",
    "        zero_grad(list(CNN_CIFAR_net.parameters()))\n",
    "        zero_grad(list(copied_model.parameters()))\n",
    "        #construct the adam-based grafting gradient \n",
    "        output1 = CNN_CIFAR_net(xt[0])\n",
    "        loss1 = loss_func(output1, yt[0])\n",
    "        loss1.backward()\n",
    "        output2 = copied_model(xt[1])\n",
    "        loss2 = loss_func(output2, yt[1])\n",
    "        loss2.backward()\n",
    "        for j, (p1, p2) in enumerate(zip(list(CNN_CIFAR_net.parameters()), list(copied_model.parameters()))):\n",
    "            d_p1 = p1.grad.data\n",
    "            d_p2 = p2.grad.data\n",
    "            if weight_decay != 0:\n",
    "                d_p1.add_(p1.data, alpha = weight_decay)\n",
    "                d_p2.add_(p2.data, alpha = weight_decay)\n",
    "            indices = torch.zeros_like(torch.clone(d_p1).detach())\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            d_p1.masked_fill_(~indices, 0)\n",
    "            d_p2.masked_fill_(indices, 0)\n",
    "            d_p1.mul_(1/b).mul_(1/prob[0])\n",
    "            d_p2.mul_(1/b).mul_(1/prob[1])\n",
    "            ggd_1 = torch.clone(d_p1).detach() + torch.clone(d_p2).detach()\n",
    "            exp_avg = h_0[j]\n",
    "            exp_avg_sq = v_0[j]\n",
    "            exp_avg.mul_(beta_1).add_(ggd_1, alpha = 1 - beta_1)\n",
    "            exp_avg_sq.mul_(beta_2).addcmul_(ggd_1, ggd_1.conj(), value = 1 - beta_2)\n",
    "            bias_correction1 = 1 - torch.pow(beta_1, (batch_idx+1))\n",
    "            bias_correction2 = 1 - torch.pow(beta_2, (batch_idx+1))\n",
    "            step_size = lr / bias_correction1\n",
    "            step_size_neg = step_size.neg()\n",
    "            bias_correction2_sqrt = bias_correction2.sqrt()\n",
    "            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(sigma / step_size_neg)\n",
    "            p1.data.addcdiv_(exp_avg, denom)\n",
    "        copied_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "        copied_model.to(device)\n",
    "        batch_idx += 1\n",
    "    CNN_CIFAR_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "    CIFAR_gadamas_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "        CIFAR_gadamas_loss_list.append(current_loss)\n",
    "        CIFAR_gadamas_test_loss_list.append(test_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_test_loader_eva, n1))\n",
    "        current_iteration =  epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9247cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training stage: training cifar-nv using cifar-10 dataset with ggd-svrg-as\n",
    "#stage one: preparation, initialization and hyperparameter setting\n",
    "CNN_CIFAR_net = CIFAR_net()\n",
    "CNN_CIFAR_net = CNN_CIFAR_net.to(device)\n",
    "Snap_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "Snap_model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#initial learning rate\n",
    "ilr0 = 100/(n ** (2/3))\n",
    "#final learning rate\n",
    "flr0 = 1/(n ** (2/3))\n",
    "#learning schedule\n",
    "lr_schedule = 'Poly'\n",
    "b = 2\n",
    "m = 128\n",
    "CIFAR_gsvrg_loss_list = []\n",
    "CIFAR_gsvrg_gradnorm_list = []\n",
    "CIFAR_gsvrg_test_loss_list = []\n",
    "\n",
    "q = 3*int(n/m) #update frequency\n",
    "\n",
    "\n",
    "#stage two: load training set \n",
    "CIFAR_train_loader = DataLoader(CIFAR_train, shuffle = True, batch_size = b*m, drop_last = True)\n",
    "CIFAR_train_loader = DeviceDataLoader(CIFAR_train_loader, device)\n",
    "\n",
    "\n",
    "#stage three: train and test\n",
    "fg = full_grad(Snap_model, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "batch_idx = 0 \n",
    "for epoch in range(epoches):\n",
    "    CNN_CIFAR_net.train()\n",
    "    for x_data, y_target in CIFAR_train_loader:\n",
    "        g0 = []\n",
    "        g1 = []\n",
    "        gf_r0 = []\n",
    "        gf_r1 = []\n",
    "        ggd = []\n",
    "        xt = []\n",
    "        yt = []\n",
    "        losst = torch.empty(2)\n",
    "        xt = x_data.split(m, dim = 0)\n",
    "        yt = y_target.split(m, dim = 0)\n",
    "        if lr_schedule == 'Poly':\n",
    "            lr = ilr0 + ((flr0 - ilr0)/( (epoches) - 1)) * int(epoch)\n",
    "        else:\n",
    "            lr = lr0\n",
    "        #construct the svrg-based grafting gradient\n",
    "        #part one: prepare for g_mb(bar{x})\n",
    "        for i, x in enumerate(xt):\n",
    "            output = Snap_model(x)\n",
    "            loss_snap = loss_func(output, yt[i])\n",
    "            loss_snap.backward()\n",
    "            for j, p in enumerate(list(Snap_model.parameters())):\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(p.data, alpha = weight_decay)\n",
    "                if i == 0:\n",
    "                    gf_r0.append(torch.clone(d_p).detach())\n",
    "                else:\n",
    "                    gf_r1.append(torch.clone(d_p).detach())\n",
    "            zero_grad(list(Snap_model.parameters()))\n",
    "        norm_2 = torch.zeros(2)\n",
    "        #deriving the sampling probability and preparing for g_mb(x^k_s)\n",
    "        for i, x in enumerate(xt):\n",
    "            output = CNN_CIFAR_net(x)\n",
    "            loss = loss_func(output, yt[i])\n",
    "            loss.backward()\n",
    "            if i == 0:\n",
    "                for z, p in zip(gf_r0, list(CNN_CIFAR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g0.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            else:\n",
    "                for z, p in zip(gf_r1, list(CNN_CIFAR_net.parameters())):\n",
    "                    d_p = p.grad.data\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(p.data, alpha = weight_decay)\n",
    "                    g1.append(torch.clone(d_p).detach())\n",
    "                    \n",
    "                    norm_2[i] = norm_2[i] + torch.sum(torch.square(torch.add(z, torch.clone(d_p).detach(), alpha = -1)))\n",
    "            zero_grad(list(CNN_CIFAR_net.parameters()))\n",
    "        if torch.min(norm_2) == 0:\n",
    "            norm_2 = torch.ones(2)\n",
    "        prob = torch.sqrt(norm_2)/torch.sum(torch.sqrt(norm_2))\n",
    "       \n",
    "        #constructing the grafting gradient \\tilde{g}^k_mb\n",
    "        for qr, qo, pr, po, fg_p in zip(gf_r0, g0, gf_r1, g1, fg):\n",
    "            indices = torch.zeros_like(qr)\n",
    "            indices = indices.bernoulli_(p = prob[0]).to(torch.bool)\n",
    "            qr.masked_fill_(~indices, 0)\n",
    "            qo.masked_fill_(~indices, 0)\n",
    "            pr.masked_fill_(indices, 0)\n",
    "            po.masked_fill_(indices, 0)\n",
    "            ggd.append(torch.add(po, qo) - torch.add(pr, qr) + fg_p)\n",
    "        \n",
    "        #update!\n",
    "        for g, p in zip(ggd, list(CNN_CIFAR_net.parameters())):\n",
    "            p.data = torch.add(p.data, g, alpha = -lr)\n",
    "        batch_idx += 1\n",
    "        #Break the loop when iteration number equal update frequency\n",
    "        if  batch_idx  % q == 0:\n",
    "            Snap_model = copy.deepcopy(CNN_CIFAR_net)\n",
    "            fg = full_grad(Snap_model, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "    CNN_CIFAR_net.eval()\n",
    "    current_gradnorm = total_grad(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "    CIFAR_gsvrg_gradnorm_list.append(current_gradnorm)\n",
    "    with torch.no_grad():\n",
    "        current_loss = total_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_train_loader_eva, n)\n",
    "        CIFAR_gsvrg_loss_list.append(current_loss)\n",
    "        CIFAR_gsvrg_test_loss_list.append(test_loss(CNN_CIFAR_net, loss_func_rec, CIFAR_test_loader_eva, n1))\n",
    "        current_iteration = epoch\n",
    "    print('Iteration: {}  Loss: {}  Gradnorm:{}'.format(current_iteration, current_loss, current_gradnorm))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
